source("D:/TY BSc CS/TY NOTES/TY-Notes/Sem 6/Practicals/DS/Prac 1 EDA/Prac 1 EDA.R")
source("D:/TY BSc CS/TY NOTES/TY-Notes/Sem 6/Practicals/DS/Prac 1 EDA/Prac 1 EDA.R")
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data
head(eda_data)
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data
head(eda_data)
summary(eda_data)
str(eda_data)
eda_data(1:8)
eda_data[1:8]
head(eda_data,3)
head(eda_data,8)
tail(eda_data,8)
eda_data[,1:5]
eda_data[3,1:5]
eda_data[1:3,1:5]
newdata1=subset(eda_data,eda_data$Education == "Grad")
newdata1
newdata2=subset(eda_data,eda_data$Age=="51" & eda_data$Gender == "M")
newdata2
a=eda_data[order(eda_data$Education),]
a
a=eda_data[order(eda_data$Education,decreasing=TRUE),]
a
a=colSums(is.na(eda_data))
a
hist(eda_data$Age)
boxplot(eda_data$Age)
mean(eda_data$Age)
min(eda_data$Age)
max(eda_data$Age)
median(eda_data$Age)
counts=table(eda_data$Education,eda_data|Gender)
counts=table(eda_data$Education,eda_data$Gender)
counts
help("pie")
plot(eda_data$Age,eda_data$Salary)
plot(eda_data$Age="p",col="green",border="red",xlim=c(20,70),ylim=c(0,70))
plot(eda_data$Age,col="green",border="red",xlim=c(20,70),ylim=c(0,70))
hist(eda_data$Age,col="green",border="red",xlim=c(20,70),ylim=c(0,70))
barplot(counts,main="Data distribution by Education v/s Gender",
col=c("blue","red"),legend.text=TRUE,xlim=c(0,10))
x=table(eda_data$Education)
per=round(x/sum(x)*100,1)
pie(x,labels=per,main="City Pie Chart", col=c("blue","red"))
pie(x,labels=per,main="City Pie Chart", col=c("saffron","red"))
pie(x,labels=per,main="City Pie Chart", col=c("olive","red"))
pie(x,labels=per,main="City Pie Chart", col=c("white","red"))
pie(x,labels=per,main="City Pie Chart", col=c("cyan","red"))
pie(x,labels=per,main="City Pie Chart", col=c("pink","red"))
pie(x,labels=per,main="City Pie Chart", col=c("peach","red"))
pie(x,labels=per,main="City Pie Chart", col=c("black","red"))
pie(x,labels=per,main="City Pie Chart", col=c("silver","red"))
pie(x,labels=per,main="City Pie Chart", col=c("gold","red"))
pie(x,labels=per,main="City Pie Chart", col=c("yellow","red"))
pie(x,labels=per,main="City Pie Chart", col=c("grey","red"))
pie(x,labels=per,main="City Pie Chart", col=c("grey","black"))
pie(x,labels=per,main="City Pie Chart", col=c("amber","black"))
pie(x,labels=per,main="City Pie Chart", col=c("red","black"))
pie(x,labels=per,main="City Pie Chart", col=c("blue","red"))
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data
head(eda_data)
summary(eda_data)
str(eda_data)
eda_data[1:8]
head(eda_data,3)
head(eda_data,8)
tail(eda_data,8)
eda_data[,1:5]
newdata1=subset(eda_data,eda_data$Education == "Grad")
newdata1
newdata2=subset(eda_data,eda_data$Age=="51" & eda_data$Gender == "M")
newdata2
a=eda_data[order(eda_data$Education),]
a
a=eda_data[order(eda_data$Education,decreasing=TRUE),]
a
a=colSums(is.na(eda_data))
a
hist(eda_data$Age)
boxplot(eda_data$Age)
mean(eda_data$Age)
min(eda_data$Age)
max(eda_data$Age)
median(eda_data$Age)
counts=table(eda_data$Education,eda_data$Gender)
counts
help("pie")
plot(eda_data$Age,eda_data$Salary)
plot(eda_data$Salary="p",col="red",main="Salary",xlim=c(0,10),ylim=c(0,10))
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data
head(eda_data)
summary(eda_data)
str(eda_data)
eda_data[1:8]
head(eda_data,3)
head(eda_data,8)
tail(eda_data,8)
eda_data[,1:5]
newdata1=subset(eda_data,eda_data$Education == "Grad")
newdata1
newdata2=subset(eda_data,eda_data$Age=="51" & eda_data$Gender == "M")
newdata2
a=eda_data[order(eda_data$Education),]
a
a=eda_data[order(eda_data$Education,decreasing=TRUE),]
a
a=colSums(is.na(eda_data))
a
hist(eda_data$Age)
boxplot(eda_data$Age)
mean(eda_data$Age)
min(eda_data$Age)
max(eda_data$Age)
median(eda_data$Age)
counts=table(eda_data$Education,eda_data$Gender)
counts
help("pie")
plot(eda_data$Age,eda_data$Salary)
plot(eda_data$Salary,type="p",col="red",main="Salary",xlim=c(0,10),ylim=c(0,10))
hist(eda_data$Age,col="green",border="red",xlim=c(20,70),ylim=c(0,70))
barplot(counts,main="Data distribution by Education v/s Gender",
col=c("blue","red"),legend.text=TRUE,xlim=c(0,10))
x=table(eda_data$Education)
per=round(x/sum(x)*100,1)
pie(x,labels=per,main="City Pie Chart", col=c("blue","red"))
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data
head(eda_data)
summary(eda_data)
str(eda_data)
eda_data[1:8]
a
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data
head(eda_data)
summary(eda_data)
str(eda_data)
eda_data[1:8]
max(eda_data$Age)
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data=read.csv(file.choose(),header=TRUE,sep=',')
eda_data
head(eda_data)
summary(eda_data)
str(eda_data)
eda_data[1:8]
head(eda_data,3)
head(eda_data,8)
tail(eda_data,8)
eda_data[,1:5]
newdata1=subset(eda_data,eda_data$Education == "Grad")
newdata1
newdata2=subset(eda_data,eda_data$Age=="51" & eda_data$Gender == "M")
newdata2
a=eda_data[order(eda_data$Education),]
a
a=eda_data[order(eda_data$Education,decreasing=TRUE),]
a
a=colSums(is.na(eda_data))
a
hist(eda_data$Age)
boxplot(eda_data$Age)
mean(eda_data$Age)
min(eda_data$Age)
max(eda_data$Age)
median(eda_data$Age)
counts=table(eda_data$Education,eda_data$Gender)
counts
help("pie")
plot(eda_data$Age,eda_data$Salary)
plot(eda_data$Salary,type="p",col="red",main="Salary",xlim=c(0,10),ylim=c(0,10))
hist(eda_data$Age,col="green",border="red",xlim=c(20,70),ylim=c(0,70))
barplot(counts,main="Data distribution by Education v/s Gender",
col=c("blue","red"),legend.text=TRUE,xlim=c(0,10))
x=table(eda_data$Education)
per=round(x/sum(x)*100,1)
pie(x,labels=per,main="City Pie Chart", col=c("blue","red"))
data1=read.csv(file.choose(),sep = ",",header=T)
t.test(data1$UK,data1$Germany,alternative="two.sided",var.equal=FALSE)
data2=read.csv(file.choose(),sep = ",",header=T)
t.test(data2$before,data2$after,alternative="greater",paired=T)
t.test(data2$before,data2$after,alternative = "less",paired = T)
t.test(data2$before,data2$after,alternative = "two.sided",paired = T)
data3<-read.csv(file.choose(),sep = ",",header=T)
cor.test(data3$mpg,data3$wt,alternative = "two.sided",method="pearson")
data4<-read.csv(file.choose(),sep=",",header=T)
t.test(data4$IQ,alternative = "greater",mu=100)
ftest=read.csv(file.choose(),sep=",",header=T)
var.test(ftest$G1,ftest$G2,alternative="two.sided")
data1=read.csv(file.choose(),sep=",",header=T)
names(data1)
summary(data1)
head(data1)
anv=aov(formula = satindex~dept,data=data1)
summary(anv)
data2=read.csv(file.choose(),sep=",",header=T)
names(data2)
summary(data2)
head(data2)
anv=aov(formula = satindex~dept+exp+dept*exp,data=data2)
data2=read.csv(file.choose(),sep=",",header=T)
names(data2)
summary(data2)
head(data2)
anv=aov(formula=satindex~dept+exp+dept*exp,data=data2)
summary(anv)
AirPassengers
#data(AirPassengers)
View(AirPassengers)
class(AirPassengers)
start(AirPassengers)
end(AirPassengers)
frequency(AirPassengers)
summary(AirPassengers)
#Exploring the data
plot(AirPassengers)#visualization of data
abline(reg=lm(AirPassengers~time(AirPassengers)))
#cycle gives positions in cycle of each observation
cycle(AirPassengers)
plot(aggregate(AirPassengers,FUN = mean))
boxplot(AirPassengers~cycle(AirPassengers))
#Conclusion:the variance and mean value in 7th(july)
#preprocessing the data
acf(AirPassengers)#spike crosses blue dotted line, data is not stationary
acf(log(AirPassengers))#to make variance stationary
acf(diff(log(AirPassengers)))#q=1 , c(p,d,q)
pacf(diff(log(AirPassengers)))#p=0
plot(diff(log(AirPassengers)))#stationary or constant Means and variance
#Auto Regression Integeration moving Average(ARIMA)model fitting
(fit<-arima(log(AirPassengers),c(0,1,1),seasonal=list(order=c(0,1,1),period=12)))
pred<-predict(fit,n.ahead=10*12)#In log form
pred1<-round(2.178^pred$pred,0)#Rounding value=e=2.718
pred1#Prediction for next 10 Year(1961-1970)
ts.plot(AirPassengers,pred1,log="y",lty=c(1,3))
AirPassengers
#data(AirPassengers)
View(AirPassengers)
class(AirPassengers)
str(AirPassengers)
start(AirPassengers)
end(AirPassengers)
frequency(AirPassengers)
summary(AirPassengers)
#Exploring the data ^
#Visulization of data
plot(AirPassengers)
abline(reg=lm(AirPassengers~time(AirPassengers)))
#Cycles gives the position in cycle of each of
cycle(AirPassengers)
plot(aggregate(AirPassengers,FUN=mean))
boxplot(AirPassengers~cycle(AirPassengers))
#Preprocessing the data
acf(AirPassengers)#Spike crosses blue dotted line
acf(log(AirPassengers))#To make variance Stationary
acf(diff(log(AirPassengers)))
acf(diff(AirPassengers))
pacf(diff(log(AirPassengers)))
plot(diff(log(AirPassengers)))
(fit=arima(log(AirPassengers),c(0,1,1),seasonal=list(order=c(0,1,1),period=12)))
pred=predict(fit,n.ahead=10*12)
pred1=round(2.718^pred$pred,0)
pred1
ts.plot(AirPassengers,pred1,log="y",lty=c(1,3))
house<-read.csv(file.choose(),sep=",",header=T)
summary(house)
names(house)
pairs(~death_rate+doctor_avail+hosp_avail+annual_income+density_per_capita,data=house)
housemodel<-lm(density_per_capita~death_rate+doctor_avail+hosp_avail+annual_income,data=house)
summary(housemodel)
index<-read.csv(file.choose(),sep=",",header=T)
names(index)
pairs(~index+written+language+tech+gk,data=index)
model1<-lm(index~.,data=index)
summary(model1)
index$pred<-fitted(model1)
head(index)
index$res<-residuals(model1)
head(index)
"to check multicolinearity"
install.packages("car")
library(car)
vif(model1)
"plot must be random indicates no heteroscedasticity"
plot(index$pred,index$res,col="red")
"errors are assumed to be normally distributed"
shapiro.test(index$res)
"detecting heteroscedasticity using ncvtest"
library(car)
ncvTest(model1,~written+language+tech+gk)
"detecting autocorelation using derbin watson test d=2(1-r)"
library(car)
durbinWatsonTest(model1)
"influence plot"
influencePlot(model1)
index=index[-33,]
"validation using k fold method "
install.packages("caret")
library("caret")
kfolds<-trainControl(method = "cv",number = 4)
modelkfold<-train(index~written+language+tech+gk,data = index,method="lm",trControl=kfolds)
modelkfold
"validation using repetative k fold"
kfoldsrp=trainControl(method="repeatedcv",number=4,repeats=5)
modelkfoldsrp=train(index~written+language+tech+gk,data=index,method="lm",trControl=kfolds)
modelkfoldsrp
"validation using leave one out"
kfoldsloocv<-trainControl(method = "LOOCV")
kfoldsloocvmodel<-train(index~written+language+tech+gk,data = index,method="lm",trControl=kfoldsloocv)
kfoldsloocvmodel
"model selection forward"
null=lm(index~1,data=index)
full=lm(index~written+language+tech+gk,data=index)
names(index)
step(null,scope=list(lower=null,upper=full),direction="forward")
"model selection backward"
step(full,scope=list(lower=null,upper=full),direction="backward")
loan=read.csv(file.choose(),header=T,sep=",")
head(loan)
summary(loan)
str(loan)
loan$AGE=as.factor(loan$AGE)
str(loan)
names(loan)
"Creating Model"
model1=glm(DEFAULTER~.,family = binomial, data = loan)
summary(model1)
"Global Testing for acceptance of the model"
null=glm(DEFAULTER~1,family=binomial,data=loan)
anova(null,model1,test="Chisq")
"predicting the probabilities"
loan$predprob=round(fitted(model1),2)
head(loan)
"classification and misclassification analysis"
install.packages("gmodels")
library(gmodels)
table(loan$DEFAULTER,fitted(model1)>0.5)
sens=95/(88+95)*100
sens
spc=478/(478+39)*100
spc
"check the trade off between sensitvity & specificity"
table(loan$DEFAULTER,fitted(model1)>0.1)
table(loan$DEFAULTER,fitted(model1)>0.2)
table(loan$DEFAULTER,fitted(model1)>0.3)
table(loan$DEFAULTER,fitted(model1)>0.4)
table(loan$DEFAULTER,fitted(model1)>0.5)
"Goodness of Fit Using Receiver Operational Curve"
pred=predict(model1,loan,type="response")
install.packages("ROCR")
library(ROCR)
rocrpred=prediction(pred,loan$DEFAULTER)
rocrperf=performance(rocrpred,"tpr","fpr")
"to check proper cut off point"
plot(rocrperf,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1))
"to check coefficients"
exp(coef(model1))
"use titanic.csv"
titanic=read.csv(file.choose(),header=T)
library(rpart)
fit=rpart(Survived ~ Pclass + Gender + Age + SibSp + Parch + Fare
+ Embarked,data=titanic, method="class")
plot(fit)
text(fit)
install.packages('rattle')
install.packages('rpart.plot')
install.packages('RColorBrewer')
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(fit)
Prediction=predict(fit,titanic,type="class")
Prediction
"k-means clustering"
iris
data("iris")
names(iris)
new_data=subset(iris,select=c(-Species))
new_data
cl=kmeans(new_data,3)
cl
data=new_data
wss=sapply(1:15,
function(k){kmeans(data,k)$tot.withinss})
plot(1:15,wss,
type="b",pch=19,frame=FALSE,
xlab="Number of Clusters k",
ylab="Total Within Clusters Sum of Squares")
install.packages("cluster")
library(cluster)
clusplot(new_data,cl$cluster,color=TRUE,shade=TRUE,labels=2,lines=0)
cl$cluster
cl$centers
"Agglomarative Clustering"
clusters=hclust(dist(iris[,3:4]))
plot(clusters)
clusterCut=cutree(clusters,3)
table(clusterCut,iris$Species)
data_iris=iris[1:4]
data_iris
Cov_data=cov(data_iris)
Cov_data
#Find out the eigenvectors and eigenvalues using the covariance matrix
Eigen_data=eigen(Cov_data)
Eigen_data
#Using the inbuilt function
PCA_data=princomp(data_iris,cor="False")
PCA_data
#Lets now compare the output variances
Eigen_data$values
PCA_data$dev^2
PCA_data$loadings[,1:4]
Eigen_data$vectors
summary(PCA_data)
biplot(PCA_data)
screeplot(PCA_data,type="lines")
#Select the first principal component for the second model
model2=PCA_data$loadings[,1]
model2
#for the second model
model2_scores=as.matrix(data_iris)%*%model2
model2_scores
#Loading Libraries for naiveBayes model
library(class)
install.packages("e1071")
library(e1071)
#Fitting the first model over the entire data
mod1=naiveBayes(iris[,1:4], iris[,5])
mod1
#Fitting the second model using the first principal component
mod2=naiveBayes(model2_scores,iris[,5])
mod2
#Accuracy for first model
table(predict(mod1,iris[,1:4]),iris[,5])
#Accuracy for second model
table(predict(mod2,model2_scores),iris[,5])
round(cor(PCA_data$score))
